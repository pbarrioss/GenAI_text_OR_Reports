{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74800a9ba874bf29a4c714b66c5f18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  66%|######5   | 1.03G/1.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13 operative reports\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d853376eb7417b817d8ac144db71a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2e9181a5ad40b4b0ee172c5033e631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 5 epochs, 13 samples, 13 batches\n",
      "Epoch 1/5, Batch 1/13, Loss: 4.4639\n",
      "Epoch 1/5, Batch 3/13, Loss: 3.6042\n",
      "Epoch 1/5, Batch 5/13, Loss: 3.0792\n",
      "Epoch 1/5, Batch 7/13, Loss: 3.3809\n",
      "Epoch 1/5, Batch 9/13, Loss: 2.7540\n",
      "Epoch 1/5, Batch 11/13, Loss: 3.5141\n",
      "Epoch 1/5, Batch 13/13, Loss: 3.2400\n",
      "Epoch 1 avg loss: 3.4005\n",
      "Epoch 2/5, Batch 1/13, Loss: 2.4145\n",
      "Epoch 2/5, Batch 3/13, Loss: 2.8441\n",
      "Epoch 2/5, Batch 5/13, Loss: 1.7898\n",
      "Epoch 2/5, Batch 7/13, Loss: 2.4411\n",
      "Epoch 2/5, Batch 9/13, Loss: 3.3419\n",
      "Epoch 2/5, Batch 11/13, Loss: 2.7281\n",
      "Epoch 2/5, Batch 13/13, Loss: 2.0990\n",
      "Epoch 2 avg loss: 2.6106\n",
      "Epoch 3/5, Batch 1/13, Loss: 1.8863\n",
      "Epoch 3/5, Batch 3/13, Loss: 1.8038\n",
      "Epoch 3/5, Batch 5/13, Loss: 2.0136\n",
      "Epoch 3/5, Batch 7/13, Loss: 2.9729\n",
      "Epoch 3/5, Batch 9/13, Loss: 1.9374\n",
      "Epoch 3/5, Batch 11/13, Loss: 2.6402\n",
      "Epoch 3/5, Batch 13/13, Loss: 2.4974\n",
      "Epoch 3 avg loss: 2.2257\n",
      "Epoch 4/5, Batch 1/13, Loss: 2.8353\n",
      "Epoch 4/5, Batch 3/13, Loss: 1.7007\n",
      "Epoch 4/5, Batch 5/13, Loss: 2.2809\n",
      "Epoch 4/5, Batch 7/13, Loss: 1.4504\n",
      "Epoch 4/5, Batch 9/13, Loss: 1.8422\n",
      "Epoch 4/5, Batch 11/13, Loss: 1.0568\n",
      "Epoch 4/5, Batch 13/13, Loss: 2.3831\n",
      "Epoch 4 avg loss: 1.9350\n",
      "Epoch 5/5, Batch 1/13, Loss: 1.0086\n",
      "Epoch 5/5, Batch 3/13, Loss: 1.4062\n",
      "Epoch 5/5, Batch 5/13, Loss: 1.9571\n",
      "Epoch 5/5, Batch 7/13, Loss: 1.6453\n",
      "Epoch 5/5, Batch 9/13, Loss: 2.1968\n",
      "Epoch 5/5, Batch 11/13, Loss: 1.4937\n",
      "Epoch 5/5, Batch 13/13, Loss: 0.8061\n",
      "Epoch 5 avg loss: 1.7005\n",
      "Model saved to ./operative-report-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Brief: Laparoscopic appendectomy for uncomplicated appendicitis in a 25-year-old patient\n",
      "Generated Report: The patient was taken to the operating room and placed supine on the table. A Foley catheter was inserted, and anesthesia was induced with IV midazolam 0. 1 mg / kg followed by fentanyl 1 mcg / kg and propofol 2 mg / min. Anesthesia was maintained with N2O and O2 delivered via an LMA. After obtaining adequate surgical conditions, a 5 mm port site was created at the level of umbilicus and insufflated to 15 mmHg pressure. An EndoGIA stapler was used to create a 10 mm port in the right lower quadrant. Once pneumoperitoneum had been achieved, CO2 insufflation was performed up to 20 mmHg. Then, a Veress needle was introduced into the abdomen through the umbilical incision. Next, two graspers were passed across the appendix, and the base of the appendix was visualized. This was done while holding it within its normal position. Two other graspers then were brought out from this area and fired along the base. All three grasper arms were removed from the abdominal cavity. Finally, the appendix stump was grasped using a vascular clamp and then transected with Endo GIA. All ports were removed under direct visualization. All trocars were removed after hemostasis was obtained. We also irrigated all peritoneal cavities and closed the fascia with interrupted 0 Vicryl sutures. The skin incisions were closed with 4 x 3 0 Monocryl subcuticular stitch. < | endoftext | > RESULTS < / > < sup > < / sup > > < mml: > MATERIAL AND METHODS < / mml = \"+ > < em > J '> < ns0: mi > N < / ns0: mn > > > ≥ 0. 05 < / math > < | quant > < < / am > | > < lt; / mrow > < ∕ > > | osci > < infra > < exp > <\" > < nasc. las > B < / m > <; / > > 〉 > < sub > < br > <.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BioGptForCausalLM, BioGptTokenizer, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"microsoft/biogpt\"\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "class OperativeReportTrainer:\n",
    "    def __init__(self, csv_path):\n",
    "        self.csv_path = csv_path\n",
    "        self.tokenizer = BioGptTokenizer.from_pretrained(MODEL_NAME)\n",
    "        self.model = BioGptForCausalLM.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "    def load_and_clean_data(self):\n",
    "        records = []\n",
    "        \n",
    "        try:\n",
    "            with open(self.csv_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            lines = content.split('\\n')\n",
    "            \n",
    "            for line in lines[1:]:  # Skip header\n",
    "                if not line.strip() or not line.startswith('\"') or 'appendectomy' not in line:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    line = line.strip()[1:-1]  # Remove outer quotes\n",
    "                    \n",
    "                    first_comma = line.find(',')\n",
    "                    if first_comma == -1:\n",
    "                        continue\n",
    "                        \n",
    "                    brief = line[:first_comma].strip()\n",
    "                    remainder = line[first_comma+1:].strip()\n",
    "                    \n",
    "                    if remainder.startswith('\"\"'):\n",
    "                        remainder = remainder[2:]\n",
    "                        last_quote_comma = remainder.rfind('\"\"')\n",
    "                        if last_quote_comma != -1:\n",
    "                            full_report = remainder[:last_quote_comma].replace('\"\"', '\"').strip()\n",
    "                            \n",
    "                            if brief and full_report:\n",
    "                                records.append({\n",
    "                                    'brief_description': brief,\n",
    "                                    'full_report': full_report,\n",
    "                                    'procedure_type': 'appendectomy'\n",
    "                                })\n",
    "                                \n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            df = pd.DataFrame(records)\n",
    "            print(f\"Loaded {len(df)} operative reports\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def create_training_prompts(self, df):\n",
    "        prompts = []\n",
    "        for _, row in df.iterrows():\n",
    "            prompt = f\"PROCEDURE: Appendectomy\\nINDICATION: {row['brief_description']}\\nOPERATIVE REPORT: {row['full_report']}\\n<|endoftext|>\"\n",
    "            prompts.append(prompt)\n",
    "        return prompts\n",
    "    \n",
    "    def prepare_dataset(self, prompts):\n",
    "        def tokenize_function(examples):\n",
    "            tokenized = self.tokenizer(\n",
    "                examples['text'],\n",
    "                truncation=True,\n",
    "                padding=False,\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors=None\n",
    "            )\n",
    "            tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "            return tokenized\n",
    "        \n",
    "        dataset = Dataset.from_dict({'text': prompts})\n",
    "        return dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "    \n",
    "    def train_model(self, dataset, output_dir='./operative-report-model', epochs=3, batch_size=2):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        self.model.train()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "        \n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "            pad_to_multiple_of=8\n",
    "        )\n",
    "        \n",
    "        from torch.utils.data import DataLoader\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "        \n",
    "        print(f\"Training {epochs} epochs, {len(dataset)} samples, {len(dataloader)} batches\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch_idx, batch in enumerate(dataloader):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                if batch_idx % 2 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx+1}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            print(f\"Epoch {epoch+1} avg loss: {total_loss / len(dataloader):.4f}\")\n",
    "        \n",
    "        self.model.save_pretrained(output_dir)\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"Model saved to {output_dir}\")\n",
    "        return self.model\n",
    "    \n",
    "    def generate_report(self, brief_description, model_path='./operative-report-model'):\n",
    "        model = BioGptForCausalLM.from_pretrained(model_path)\n",
    "        tokenizer = BioGptTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        prompt = f\"PROCEDURE: Appendectomy\\nINDICATION: {brief_description}\\nOPERATIVE REPORT:\"\n",
    "        inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        attention_mask = torch.ones_like(inputs)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=min(len(inputs[0]) + 400, 1024),\n",
    "                temperature=0.3,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                top_k=50,\n",
    "                repetition_penalty=1.2,\n",
    "                no_repeat_ngram_size=3,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                num_return_sequences=1,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        report_start = generated_text.find(\"OPERATIVE REPORT:\") + len(\"OPERATIVE REPORT:\")\n",
    "        generated_report = generated_text[report_start:].strip()\n",
    "        \n",
    "        return self._clean_generated_text(generated_report)\n",
    "    \n",
    "    def _clean_generated_text(self, text):\n",
    "        lines = text.split('.')\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and len(line) > 10:\n",
    "                is_repetitive = any(line in prev_line or prev_line in line for prev_line in cleaned_lines[-3:])\n",
    "                if not is_repetitive:\n",
    "                    cleaned_lines.append(line)\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "        return '. '.join(cleaned_lines) + '.' if cleaned_lines else text\n",
    "\n",
    "def main():\n",
    "    trainer = OperativeReportTrainer('or_reports.csv')\n",
    "    \n",
    "    df = trainer.load_and_clean_data()\n",
    "    if len(df) == 0:\n",
    "        print(\"No data loaded! Check CSV format.\")\n",
    "        return\n",
    "    \n",
    "    prompts = trainer.create_training_prompts(df)\n",
    "    dataset = trainer.prepare_dataset(prompts)\n",
    "    \n",
    "    trainer.train_model(dataset, epochs=5, batch_size=1)\n",
    "    \n",
    "    test_brief = \"Laparoscopic appendectomy for uncomplicated appendicitis in a 25-year-old patient\"\n",
    "    generated_report = trainer.generate_report(test_brief)\n",
    "    \n",
    "    print(f\"\\nTest Brief: {test_brief}\")\n",
    "    print(f\"Generated Report: {generated_report}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST 1: Laparoscopic appendectomy for acute appendicitis in a 30-year-old male\n",
      "============================================================\n",
      "The patient was taken to the operating room, placed supine on his left side. A midline incision was made and performed under direct vision. After adequate visualization of the appendix, it was identified as an abnormal structure between the cecum and mesoappendix. It was then dissected out from its base with a vascular stapler. Once hemostasis had been obtained, the appendix was grasped with a hemostatic clamp and divided using a vascular bipolar device. Next, the umbilical fascia was incised longitudinally. The peritoneum was entered through this area and the appendix ligated with a Vascular Bipolar Device. All other ports were removed under direct visualization without difficulty. The abdomen was insufflated at 15 mmHg pressure until CO2 insufflation pressures reached 25 mmHg. Subsequently, the abdomen was closed with interrupted 0 Vicryl sutures across all layers. Steri-Strips were applied over the skin incisions. The umbilicus was approximated with 2-0 Monocryl subcuticular stitch. All dressings were changed after surgery. The patient tolerated the procedure well. He returned to recovery in stable condition. On postoperative day 1, he was extubated in good condition. His pain was controlled with IV boluses of morphine sulfate and he required no further doses of narcotics. At home, he is tolerating clear liquids and has resumed normal activities of daily living. He will be followed closely and seen frequently. Follow-up imaging studies are being done. He is doing well. CONCLUSION: This case demonstrates that laparoscopic appendectomy can be safely utilized in patients with complicated appendicitis. We believe this technique should be considered when treating such cases. The benefits include decreased operative time, less blood loss, shorter hospital stay, improved cosmesis, and faster return to work / school. Our experience suggests that this approach may also decrease morbidity associated with open appendectomy. The use of this technique does not require conversion to open appendectomy or additional port placement. In our opinion, we have found this technique safe and effective.\n",
      "\n",
      "\n",
      "============================================================\n",
      "TEST 2: Open appendectomy for perforated appendicitis with peritonitis\n",
      "============================================================\n",
      "The patient was taken to the operating room and placed supine on a surgical table. A midline incision was made, and after obtaining adequate visualization of the abdomen, an infraumbilical incision was performed. After insufflation of CO2 at 10 mmHg pressure in the abdominal cavity, the fascia was incised longitudinally. Perforation of the appendix was confirmed intraoperatively. Peritoneal lavage was done using normal saline solution (4 liters). All other intraabdominal contents were aspirated. Appendix was easily visualized and divided between two staples. Next, the mesoappendix was ligated with vascular clips. Then, the stump was closed with interrupted 0 Vicryl sutures. The wound was irrigated with warm sterile saline solution. Steri-Strips were applied around all areas. The peritoneum was then closed with 2-0 Monocryl subcuticular stitch. Finally, the skin was closed by applying Dermabond. The patient tolerated this well without any complications. She was extubated immediately postoperatively and returned to recovery area within 15 minutes. On postoperative day 1, she had no evidence of infection or dehiscence. Her wounds were dressed with Steri Sant'ze dressings and discharged home in stable condition. On follow-up phone call, 12 hours later, her wounds were clean and dry. At 24-hour follow-up. Figure 1Operative report shows the operative video. Table of key points. Text is shown in text. Figure 1Video demonstrates the operative steps. 2Operative images showing the operative field. The following are shown below. Figure 2Operative radiographs show the position of the staples and clips. The image showing the suture line is shown outlining the x-axis and y-axis. The figure showing the clip line is displayed under the background of the image. The picture showing the staple lines is shown under the same background as the image showing them. The above figures are accompanied by accompanying videos that demonstrate the advantages and disadvantages of each technique. The final pathology reports showed purulent material along the appendiceal stump and free.\n",
      "\n",
      "\n",
      "============================================================\n",
      "TEST 3: Emergency appendectomy for gangrenous appendicitis\n",
      "============================================================\n",
      "The patient was taken to the operating room and placed supine on a surgical table. A midline incision was made, and under direct vision, an infraumbilical incision was performed. After adequate hemostasis had been obtained, the fascia was incised with a blunt dissector. All bleeding from the abdominal cavity was controlled. The mesoappendix was divided with a vascular stapler and then ligated proximally. No blood loss or leakage of contents into the peritoneal cavity occurred. The peritoneum was irrigated out. The wound was closed in layers using interrupted 0 Vicryl sutures. Steri-Strips were applied subcuticular at both ends of each stitch. There was no evidence of dehiscence of the closure. The skin was closed with 4-0 Monocryl. The patient tolerated this well without any complications. She was extubated immediately after her procedure and returned to recovery in stable condition. He was discharged home in good clinical condition. On postoperative day 2, he was noted to have minimal pain in his right lower quadrant and was started on oral antibiotics. Within hours of discharge, he developed signs of dehydration that warranted further evaluation by the critical care team. We proceeded directly to the intensive care unit where we found him to be septic on physical examination and laboratory testing. Final cultures revealed methicillin resistant Staphylococcus aureus (MRSA). His wounds were treated as per our institutional protocol and he received IV vancomycin and clindamycin. Antibiotics were continued until cultures became sterile. The next morning, he underwent an elective laparoscopic appendectomy. The operative time was approximately 90 minutes and there was no intraoperative complication. The abdomen was prepped and draped in sterile fashion. The port sites were infiltrated with 0. 25% Marcaine and dressed with Steri Seal. The fascia was approximated with # 5-0 Vicuron suture. The umbilicus was closed primarily with 4 x 4 0 Mono Vicryl suture. There were no signs of infection postoperatively. The total hospital course was unremarkable. The incision was closed intracorporeally with # 3-0 Marcaine.\n",
      "\n",
      "\n",
      "============================================================\n",
      "TEST 4: Laparoscopic appendectomy without perforation in young patient\n",
      "============================================================\n",
      "The patient was prepped and draped in sterile fashion. A Foley catheter was inserted for bladder decompression prior to incision of the abdomen. After obtaining an adequate general endotracheal anesthetic, a 5-mm port placed at the level of umbilicus under direct vision. A Veress needle was introduced into the right lower quadrant. An EndoGIA stapler was used to create a 10-mm umbilical defect. Pneumoperitoneum was performed with CO2 insufflation up to 15 mmHg. Once hemostasis had been obtained, the fascia was incised along its entire length. Next, a 12-mm trocar was inserted suprapubic. Then, two additional 5-0 Vicryl subcuticular sutures were applied across the midline. Finally, the appendix stump was ligated using the Endo GIA device. All ports were removed under direct visualization. The peritoneum was irrigated with warm saline solution. A 4 x 2 cm area of the anterior abdominal wall was explored through which no bleeding or leakage from the wound occurred. The skin incisions were closed with 3-0 Monocryl subcostal stitch. The fascia was approximated with interrupted 0 Vicia monofilament suture. The remaining layers were approximated with Dermabond. Steri-Strips. The peritoneal cavity was irrigated and then all drains were removed. The patient tolerated this well. She was extubated immediately after surgery and returned to recovery room in stable condition. On postoperative day 1, she was taken back to recovery ward where there were no signs of infection. On follow-up examination on postoperative day 7, her physical exam revealed normal vital signs, and she was tolerating clear liquids and solid food orally. At home, she reported that she is asymptomatic and has resumed regular activities of daily living within days of discharge. The following morning, the patient's pain score was 0 / 10 and she will be followed closely in our outpatient clinic. The next morning, we take blood samples for complete blood count (CBC), C-reactive protein (CRP), and procalcitonin.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test generation without GUI\n",
    "import torch\n",
    "from transformers import BioGptForCausalLM, BioGptTokenizer\n",
    "\n",
    "class SimpleReportGenerator:\n",
    "    def __init__(self, model_path='./operative-report-model'):\n",
    "        self.model = BioGptForCausalLM.from_pretrained(model_path)\n",
    "        self.tokenizer = BioGptTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    def generate_report(self, brief_description, temperature=0.3, max_length=400):\n",
    "        \"\"\"Generate operative report with improved parameters\"\"\"\n",
    "        prompt = f\"PROCEDURE: Appendectomy\\nINDICATION: {brief_description}\\nOPERATIVE REPORT:\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors='pt')\n",
    "        \n",
    "        # Attention mask: tells model which tokens to pay attention to (1=real token, 0=padding)\n",
    "        attention_mask = torch.ones_like(inputs)\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for faster inference\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=min(len(inputs[0]) + max_length, 1024),\n",
    "                \n",
    "                # Sampling parameters:\n",
    "                temperature=temperature,  # 0.3 = more focused/conservative, 1.0 = more random/creative\n",
    "                do_sample=True,          # Enable sampling (vs greedy search)\n",
    "                \n",
    "                # Top-p (nucleus sampling): Only consider tokens that make up top 90% probability mass\n",
    "                top_p=0.9,               \n",
    "                \n",
    "                # Top-k: Only consider the 50 most likely next tokens at each step\n",
    "                top_k=50,                \n",
    "                \n",
    "                repetition_penalty=1.2,   # Penalize repeated tokens (1.0 = no penalty, >1.0 = discourage)\n",
    "                no_repeat_ngram_size=3,   # Don't repeat any 3-word sequences\n",
    "                \n",
    "                # ✅ IMPROVED STOPPING CRITERIA:\n",
    "                pad_token_id=self.tokenizer.eos_token_id,  # Use end-of-sequence as padding\n",
    "                eos_token_id=self.tokenizer.eos_token_id,  # Force stop at EOS\n",
    "                early_stopping=True,       # Stop when end-of-sequence token is generated\n",
    "                \n",
    "                # ✅ BLOCK HTML/XML GARBAGE TOKENS:\n",
    "                bad_words_ids=[\n",
    "                    self.tokenizer.encode(\"<\", add_special_tokens=False),\n",
    "                    self.tokenizer.encode(\">\", add_special_tokens=False),\n",
    "                    self.tokenizer.encode(\"endoftext\", add_special_tokens=False),\n",
    "                    self.tokenizer.encode(\"AbstractText\", add_special_tokens=False),\n",
    "                    self.tokenizer.encode(\"NlmCategory\", add_special_tokens=False),\n",
    "                    self.tokenizer.encode(\"UNASSIGNED\", add_special_tokens=False),\n",
    "                    self.tokenizer.encode(\"ns0:\", add_special_tokens=False),\n",
    "                    self.tokenizer.encode(\"mml:\", add_special_tokens=False)\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # ✅ EARLY GARBAGE DETECTION: Cut at first sign of corruption\n",
    "        garbage_tokens = [\"<\", \"endoftext\", \"AbstractText\", \"NlmCategory\", \"UNASSIGNED\", \"ns0:\", \"mml:\", \"≥\", \"≤\"]\n",
    "        if any(token in generated_text for token in garbage_tokens):\n",
    "            # Find first garbage token and cut there\n",
    "            cut_points = []\n",
    "            for token in garbage_tokens:\n",
    "                if token in generated_text:\n",
    "                    cut_points.append(generated_text.find(token))\n",
    "            if cut_points:\n",
    "                generated_text = generated_text[:min(cut_points)]\n",
    "        \n",
    "        report_start = generated_text.find(\"OPERATIVE REPORT:\") + len(\"OPERATIVE REPORT:\")\n",
    "        generated_report = generated_text[report_start:].strip()\n",
    "        \n",
    "        return self._clean_repetitive_text(generated_report)\n",
    "    \n",
    "    def _clean_repetitive_text(self, text):\n",
    "        \"\"\"Remove repetitive sentences\"\"\"\n",
    "        sentences = text.split('.')\n",
    "        cleaned = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if sentence and len(sentence) > 10:\n",
    "                is_repetitive = False\n",
    "                for prev in cleaned[-2:]:\n",
    "                    if sentence in prev or prev in sentence:\n",
    "                        is_repetitive = True\n",
    "                        break\n",
    "                \n",
    "                if not is_repetitive:\n",
    "                    cleaned.append(sentence)\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "        return '. '.join(cleaned) + '.' if cleaned else text\n",
    "\n",
    "# Test the generator\n",
    "def test_generation():\n",
    "    generator = SimpleReportGenerator()\n",
    "    \n",
    "    test_cases = [\n",
    "        \"Laparoscopic appendectomy for acute appendicitis in a 30-year-old male\",\n",
    "        \"Open appendectomy for perforated appendicitis with peritonitis\",\n",
    "        \"Emergency appendectomy for gangrenous appendicitis\",\n",
    "        \"Laparoscopic appendectomy without perforation in young patient\"\n",
    "    ]\n",
    "    \n",
    "    for i, brief in enumerate(test_cases, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TEST {i}: {brief}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        report = generator.generate_report(brief, temperature=0.3)\n",
    "        print(report)\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_generation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
